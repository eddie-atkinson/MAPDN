{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook processes the input data for the simulation of an Australian distribution network feeder. Load and generation data is sourced from the [Pecan Street](https://www.pecanstreet.org/) student-licensed data set. Power factor data is sourced from the [ECO](https://www.vs.inf.ethz.ch/res/show.html?what=eco-data) data set and combined with the Pecan Street data to produce active and reactive power load profiles.\n",
    "\n",
    "Network models from the [Australian Low Voltage Feeder Taxonomy](https://near.csiro.au/assets/f325fb3c-2dcd-410c-97a8-e55dc68b8064) are converted from OpenDSS format to pandapower. \n",
    "\n",
    "The overall data cleaning process is depicted below:\n",
    "\n",
    "![data_cleaning](https://i.ibb.co/2KZ48xg/data-cleaning.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Processing\n",
    "The load data was downloaded from the Pecan Street Dataport. The original files were called `1s_data_austin_file{1,2,3,4}.csv.gz`. \n",
    "\n",
    "\n",
    "Each is unzipped and processed. The single file containing all households is split into a file per household containing only the `dataid`, `localminute`, `grid`, `solar` and `solar2` columns. This reduces the size of the data set, and makes it practical to manipulate the data from a single household at once in pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from typing import Union\n",
    "from datetime import datetime\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from scipy import io, stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PECAN_ST_FILENAMES = [\n",
    "    \"1s_data_austin_file1.csv.gz\",\n",
    "    \"1s_data_austin_file2.csv.gz\",\n",
    "    \"1s_data_austin_file3.csv.gz\",\n",
    "    \"1s_data_austin_file4.csv.gz\",\n",
    "]\n",
    "\n",
    "INPUT_DATA_PATH = Path(\"./input_data\")\n",
    "OUTPUT_DATA_PATH = Path(\"./output_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_household_file(file_path: Path, header_row: str):\n",
    "    with open(file_path, \"w\") as fd:\n",
    "        fd.write(header_row)\n",
    "\n",
    "\n",
    "def process_file(file_path: Path):\n",
    "    file_descriptors = {}\n",
    "    header_row = \"dataid,localminute,grid,solar,solar2\\n\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as infile:\n",
    "            csv_reader = csv.DictReader(infile)\n",
    "            for row in csv_reader:\n",
    "                dataid = row[\"dataid\"]\n",
    "                localminute = row[\"localminute\"]\n",
    "                grid = row[\"grid\"]\n",
    "                solar = row[\"solar\"]\n",
    "                solar2 = row[\"solar2\"]\n",
    "\n",
    "                file_descriptor = file_descriptors.get(dataid, None)\n",
    "                if not file_descriptor:\n",
    "                    output_file_path = OUTPUT_DATA_PATH / f\"{dataid}-load-pv.csv\"\n",
    "                    if not output_file_path.exists():\n",
    "                        create_household_file(output_file_path, header_row)\n",
    "                    file_descriptor = open(output_file_path, \"a\")\n",
    "                    file_descriptors[dataid] = file_descriptor\n",
    "                file_descriptor.write(\n",
    "                    f\"{dataid},{localminute},{grid},{solar},{solar2}\\n\",\n",
    "                )\n",
    "    finally:\n",
    "        for fd in file_descriptors.values():\n",
    "            fd.close()\n",
    "\n",
    "\n",
    "def unzip_raw_data(data_path: Path, unzip_path: Path):\n",
    "    if not unzip_path.exists():\n",
    "        # Lord forgive me for this but Python is terribly slow at this\n",
    "        os.system(f\"gzip -d < {data_path.resolve()} > {unzip_path.resolve()}\")\n",
    "        print(\"unzipped\")\n",
    "        \n",
    "\n",
    "\n",
    "def process_data(data_path: Path):\n",
    "    outfile_name = data_path.stem\n",
    "    unzipped_path = INPUT_DATA_PATH / outfile_name\n",
    "    unzip_raw_data(data_path, unzipped_path)\n",
    "    process_file(unzipped_path)\n",
    "    unzipped_path.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s_data_austin_file1.csv.gz\n",
      "unzipped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: /Users/eddie/Documents/Uni/MAPDN/data_preparation/input_data/1s_data_austin_file1.csv.gz: No such file or directory\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input_data/1s_data_austin_file1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(file_name)\n\u001b[1;32m      6\u001b[0m data_path \u001b[38;5;241m=\u001b[39m INPUT_DATA_PATH \u001b[38;5;241m/\u001b[39m file_name\n\u001b[0;32m----> 7\u001b[0m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m unzipped_path \u001b[38;5;241m=\u001b[39m INPUT_DATA_PATH \u001b[38;5;241m/\u001b[39m outfile_name\n\u001b[1;32m     45\u001b[0m unzip_raw_data(data_path, unzipped_path)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43munzipped_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m unzipped_path\u001b[38;5;241m.\u001b[39munlink()\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m header_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataid,localminute,grid,solar,solar2\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[1;32m     11\u001b[0m         csv_reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(infile)\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m csv_reader:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input_data/1s_data_austin_file1.csv'"
     ]
    }
   ],
   "source": [
    "# NOTE: This takes a long time to run, 1.5 hours on an M1 mac.\n",
    "# I could probably optimise it, but I'm only running it once \n",
    "# Should probably also multi thread it\n",
    "for file_name in PECAN_ST_FILENAMES:\n",
    "    print(file_name)\n",
    "    data_path = INPUT_DATA_PATH / file_name\n",
    "    process_data(data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate to 30s resolution\n",
    "There's still a lot of data to work with. Though a resolution of 1s is interesting, the volume of data is too large for us to work with sensibly. Let's downsample the data to 30s resolution to make it a bit easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_interval(dt):\n",
    "    seconds = dt.second\n",
    "    interval_args = {\n",
    "        \"year\": dt.year,\n",
    "        \"month\": dt.month,\n",
    "        \"day\": dt.day,\n",
    "        \"hour\": dt.hour,\n",
    "        \"minute\": dt.minute,\n",
    "    }\n",
    "\n",
    "    if seconds < 30:\n",
    "        return datetime(**interval_args, second=0)\n",
    "    else:\n",
    "        return datetime(**interval_args, second=30)\n",
    "\n",
    "\n",
    "def calculate_use(grid: str, solar: str, solar2: str) -> Union[float, None]:\n",
    "    if not grid:\n",
    "        return None\n",
    "    grid_val = float(grid)\n",
    "    solar_val = float(solar) if solar else 0\n",
    "    solar2_val = float(solar2) if solar2 else 0\n",
    "\n",
    "    # This may seem weird but it's explained here: https://docs.google.com/document/d/1_9H9N4cgKmJho7hK8nii6flIGKPycL7tlWEtd4UhVEQ/edit#\n",
    "    return grid_val + solar_val + solar2_val\n",
    "\n",
    "\n",
    "def calculate_net_solar(solar: str, solar2: str) -> Union[float, None]:\n",
    "    if not solar and not solar2:\n",
    "        return None\n",
    "    solar_val = float(solar) if solar else 0\n",
    "    solar2_val = float(solar) if solar2 else 0\n",
    "\n",
    "    return solar_val + solar2_val\n",
    "\n",
    "\n",
    "def downsample_data(file_path: Path, out_path: Path):\n",
    "    data_periods = {}\n",
    "    with open(file_path, \"r\") as infile:\n",
    "        csv_reader = csv.DictReader(infile)\n",
    "        for line in csv_reader:\n",
    "            # The last three characters are TZ info, we will lose an hour's data every time the clocks change.\n",
    "            # Fortunately, we don't really care about that\n",
    "            localminute = line[\"localminute\"][:-3]\n",
    "            grid = line[\"grid\"]\n",
    "            solar = line[\"solar\"]\n",
    "            solar2 = line[\"solar2\"]\n",
    "            solar_val = calculate_net_solar(solar, solar2)\n",
    "            use = calculate_use(grid, solar, solar2)\n",
    "            dt = datetime.strptime(localminute, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            interval = get_closest_interval(dt)\n",
    "            if not data_periods.get(interval, None):\n",
    "                data_periods[interval] = dict(\n",
    "                    solar_sum=0, solar_count=0, use_sum=0, use_count=0\n",
    "                )\n",
    "\n",
    "            data = data_periods[interval]\n",
    "            if solar_val is not None:\n",
    "                data[\"solar_sum\"] += solar_val\n",
    "                data[\"solar_count\"] += 1\n",
    "            if use is not None:\n",
    "                data[\"use_sum\"] += use\n",
    "                data[\"use_count\"] += 1\n",
    "\n",
    "    intervals = sorted(data_periods)\n",
    "    with open(out_path, \"w\") as outfile:\n",
    "        header = \"datetime,solar,use\\n\"\n",
    "        outfile.write(header)\n",
    "        for interval in intervals:\n",
    "            interval_data = data_periods[interval]\n",
    "            iso_date = interval.isoformat()\n",
    "            solar_count = interval_data[\"solar_count\"]\n",
    "            solar_sum = interval_data[\"solar_sum\"]\n",
    "\n",
    "            use_count = interval_data[\"use_count\"]\n",
    "            use_sum = interval_data[\"use_sum\"]\n",
    "\n",
    "            solar_val = \"\"\n",
    "            use_val = \"\"\n",
    "            if solar_count:\n",
    "                solar_val = solar_sum / solar_count\n",
    "            if use_count:\n",
    "                use_val = use_sum / use_count\n",
    "            outfile.write(f\"{iso_date},{solar_val},{use_val}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This also takes ages - should really have written this more effficiently\n",
    "threads = 25\n",
    "t = ThreadPool(threads)\n",
    "\n",
    "files_to_process = []\n",
    "for file_path in OUTPUT_DATA_PATH.iterdir():\n",
    "    if not str(file_path).endswith(\"load-pv.csv\"):\n",
    "        continue\n",
    "    outfile_name = f\"{file_path.stem}-reduced.csv\"\n",
    "    outfile_path = file_path.parent / outfile_name\n",
    "    files_to_process.append((\n",
    "        file_path,\n",
    "        outfile_path\n",
    "    ))\n",
    " \n",
    "\n",
    "with ThreadPool(threads) as t:\n",
    "    t.starmap(downsample_data, files_to_process)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further processing\n",
    "The next steps are:\n",
    "1. Aggregate the ECO dataset and compute an average power factor.\n",
    "2. Check the existing load and PV data for outliers and replace them with neighbouring values. We know from experience that these values exist. \n",
    "3. Perturb the average power factor by +/- 10% and compute the appropriate reactive power for each interval in the load data.\n",
    "4. Bootstrap to 100 load and PV profiles\n",
    "5. Create active, reactive, and pv profiles for all the households.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data was requested from ETH Zurich and downloaded directly from their portal\n",
    "adres_input_path = INPUT_DATA_PATH / \"ADRES_Daten_120208.mat\"\n",
    "loaded = io.loadmat(adres_input_path)\n",
    "adres_df = pd.DataFrame(loaded[\"Data\"][\"PQ\"][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.200001</td>\n",
       "      <td>-14.670000</td>\n",
       "      <td>22.150000</td>\n",
       "      <td>3.189000</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>-80.730003</td>\n",
       "      <td>550.083313</td>\n",
       "      <td>-31.236666</td>\n",
       "      <td>247.800003</td>\n",
       "      <td>66.066666</td>\n",
       "      <td>...</td>\n",
       "      <td>168.100006</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>-31.100000</td>\n",
       "      <td>52.200001</td>\n",
       "      <td>-42.900002</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>-1.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.500000</td>\n",
       "      <td>-14.980000</td>\n",
       "      <td>22.180000</td>\n",
       "      <td>3.529000</td>\n",
       "      <td>140.699997</td>\n",
       "      <td>-80.550003</td>\n",
       "      <td>549.080017</td>\n",
       "      <td>-31.166000</td>\n",
       "      <td>247.960007</td>\n",
       "      <td>66.199997</td>\n",
       "      <td>...</td>\n",
       "      <td>168.500000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>10.640000</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>54.900002</td>\n",
       "      <td>-31.180000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>-43.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>-1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.799999</td>\n",
       "      <td>-15.120000</td>\n",
       "      <td>22.040001</td>\n",
       "      <td>3.408000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>-80.330002</td>\n",
       "      <td>549.099976</td>\n",
       "      <td>-31.172001</td>\n",
       "      <td>247.880005</td>\n",
       "      <td>65.839996</td>\n",
       "      <td>...</td>\n",
       "      <td>168.100006</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>10.810000</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>55.799999</td>\n",
       "      <td>-31.299999</td>\n",
       "      <td>52.900002</td>\n",
       "      <td>-43.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>-1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.900002</td>\n",
       "      <td>-14.660000</td>\n",
       "      <td>22.219999</td>\n",
       "      <td>2.956000</td>\n",
       "      <td>141.100006</td>\n",
       "      <td>-80.550003</td>\n",
       "      <td>540.880005</td>\n",
       "      <td>-31.386000</td>\n",
       "      <td>247.399994</td>\n",
       "      <td>65.739998</td>\n",
       "      <td>...</td>\n",
       "      <td>168.500000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>10.710000</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>60.200001</td>\n",
       "      <td>-31.660000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>-42.900002</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>-1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.700001</td>\n",
       "      <td>-14.860000</td>\n",
       "      <td>22.120001</td>\n",
       "      <td>3.148000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>-80.669998</td>\n",
       "      <td>428.760010</td>\n",
       "      <td>-29.719999</td>\n",
       "      <td>247.259995</td>\n",
       "      <td>65.699997</td>\n",
       "      <td>...</td>\n",
       "      <td>168.399994</td>\n",
       "      <td>20.700001</td>\n",
       "      <td>10.650000</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>60.200001</td>\n",
       "      <td>-31.740000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>-42.900002</td>\n",
       "      <td>35.099998</td>\n",
       "      <td>-1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209595</th>\n",
       "      <td>102.300003</td>\n",
       "      <td>24.510000</td>\n",
       "      <td>133.800003</td>\n",
       "      <td>-62.160000</td>\n",
       "      <td>108.300003</td>\n",
       "      <td>54.270000</td>\n",
       "      <td>290.899994</td>\n",
       "      <td>17.570000</td>\n",
       "      <td>107.699997</td>\n",
       "      <td>40.610001</td>\n",
       "      <td>...</td>\n",
       "      <td>139.899994</td>\n",
       "      <td>-9.500000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>2.300</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>-7.800000</td>\n",
       "      <td>34.200001</td>\n",
       "      <td>-27.740000</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>-27.469999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209596</th>\n",
       "      <td>102.500000</td>\n",
       "      <td>24.459999</td>\n",
       "      <td>133.800003</td>\n",
       "      <td>-62.169998</td>\n",
       "      <td>113.400002</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>289.799988</td>\n",
       "      <td>16.240000</td>\n",
       "      <td>107.699997</td>\n",
       "      <td>40.669998</td>\n",
       "      <td>...</td>\n",
       "      <td>139.800003</td>\n",
       "      <td>-9.500000</td>\n",
       "      <td>221.699997</td>\n",
       "      <td>2.300</td>\n",
       "      <td>57.200001</td>\n",
       "      <td>-7.900000</td>\n",
       "      <td>34.200001</td>\n",
       "      <td>-27.889999</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>-27.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209597</th>\n",
       "      <td>102.699997</td>\n",
       "      <td>24.549999</td>\n",
       "      <td>133.800003</td>\n",
       "      <td>-62.349998</td>\n",
       "      <td>112.300003</td>\n",
       "      <td>54.090000</td>\n",
       "      <td>287.299988</td>\n",
       "      <td>13.430000</td>\n",
       "      <td>107.900002</td>\n",
       "      <td>40.720001</td>\n",
       "      <td>...</td>\n",
       "      <td>139.800003</td>\n",
       "      <td>-9.600000</td>\n",
       "      <td>221.800003</td>\n",
       "      <td>2.400</td>\n",
       "      <td>56.299999</td>\n",
       "      <td>-7.700000</td>\n",
       "      <td>34.200001</td>\n",
       "      <td>-27.820000</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>-27.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209598</th>\n",
       "      <td>102.500000</td>\n",
       "      <td>24.389999</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>-62.290001</td>\n",
       "      <td>108.199997</td>\n",
       "      <td>54.189999</td>\n",
       "      <td>282.299988</td>\n",
       "      <td>11.320000</td>\n",
       "      <td>107.599998</td>\n",
       "      <td>40.020000</td>\n",
       "      <td>...</td>\n",
       "      <td>139.899994</td>\n",
       "      <td>-9.500000</td>\n",
       "      <td>221.500000</td>\n",
       "      <td>2.400</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>-7.700000</td>\n",
       "      <td>34.299999</td>\n",
       "      <td>-27.180000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>-27.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209599</th>\n",
       "      <td>102.199997</td>\n",
       "      <td>24.530001</td>\n",
       "      <td>134.100006</td>\n",
       "      <td>-62.279999</td>\n",
       "      <td>108.199997</td>\n",
       "      <td>54.189999</td>\n",
       "      <td>280.500000</td>\n",
       "      <td>10.540000</td>\n",
       "      <td>107.699997</td>\n",
       "      <td>39.919998</td>\n",
       "      <td>...</td>\n",
       "      <td>139.899994</td>\n",
       "      <td>-9.300000</td>\n",
       "      <td>221.600006</td>\n",
       "      <td>2.500</td>\n",
       "      <td>56.299999</td>\n",
       "      <td>-7.600000</td>\n",
       "      <td>34.299999</td>\n",
       "      <td>-26.719999</td>\n",
       "      <td>28.299999</td>\n",
       "      <td>-27.670000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1209600 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0          1           2          3           4          5    \\\n",
       "0         40.200001 -14.670000   22.150000   3.189000  141.500000 -80.730003   \n",
       "1         41.500000 -14.980000   22.180000   3.529000  140.699997 -80.550003   \n",
       "2         40.799999 -15.120000   22.040001   3.408000  141.000000 -80.330002   \n",
       "3         40.900002 -14.660000   22.219999   2.956000  141.100006 -80.550003   \n",
       "4         40.700001 -14.860000   22.120001   3.148000  141.000000 -80.669998   \n",
       "...             ...        ...         ...        ...         ...        ...   \n",
       "1209595  102.300003  24.510000  133.800003 -62.160000  108.300003  54.270000   \n",
       "1209596  102.500000  24.459999  133.800003 -62.169998  113.400002  54.000000   \n",
       "1209597  102.699997  24.549999  133.800003 -62.349998  112.300003  54.090000   \n",
       "1209598  102.500000  24.389999  134.000000 -62.290001  108.199997  54.189999   \n",
       "1209599  102.199997  24.530001  134.100006 -62.279999  108.199997  54.189999   \n",
       "\n",
       "                6          7           8          9    ...         170  \\\n",
       "0        550.083313 -31.236666  247.800003  66.066666  ...  168.100006   \n",
       "1        549.080017 -31.166000  247.960007  66.199997  ...  168.500000   \n",
       "2        549.099976 -31.172001  247.880005  65.839996  ...  168.100006   \n",
       "3        540.880005 -31.386000  247.399994  65.739998  ...  168.500000   \n",
       "4        428.760010 -29.719999  247.259995  65.699997  ...  168.399994   \n",
       "...             ...        ...         ...        ...  ...         ...   \n",
       "1209595  290.899994  17.570000  107.699997  40.610001  ...  139.899994   \n",
       "1209596  289.799988  16.240000  107.699997  40.669998  ...  139.800003   \n",
       "1209597  287.299988  13.430000  107.900002  40.720001  ...  139.800003   \n",
       "1209598  282.299988  11.320000  107.599998  40.020000  ...  139.899994   \n",
       "1209599  280.500000  10.540000  107.699997  39.919998  ...  139.899994   \n",
       "\n",
       "               171         172    173        174        175        176  \\\n",
       "0        20.600000   10.700000 -0.995  55.000000 -31.100000  52.200001   \n",
       "1        20.600000   10.640000 -0.947  54.900002 -31.180000  52.500000   \n",
       "2        20.600000   10.810000 -0.945  55.799999 -31.299999  52.900002   \n",
       "3        20.600000   10.710000 -0.962  60.200001 -31.660000  52.500000   \n",
       "4        20.700001   10.650000 -0.927  60.200001 -31.740000  52.500000   \n",
       "...            ...         ...    ...        ...        ...        ...   \n",
       "1209595  -9.500000  222.000000  2.300  57.500000  -7.800000  34.200001   \n",
       "1209596  -9.500000  221.699997  2.300  57.200001  -7.900000  34.200001   \n",
       "1209597  -9.600000  221.800003  2.400  56.299999  -7.700000  34.200001   \n",
       "1209598  -9.500000  221.500000  2.400  56.500000  -7.700000  34.299999   \n",
       "1209599  -9.300000  221.600006  2.500  56.299999  -7.600000  34.299999   \n",
       "\n",
       "               177        178        179  \n",
       "0       -42.900002  35.000000  -1.700000  \n",
       "1       -43.000000  35.000000  -1.800000  \n",
       "2       -43.000000  35.000000  -1.800000  \n",
       "3       -42.900002  35.000000  -1.800000  \n",
       "4       -42.900002  35.099998  -1.900000  \n",
       "...            ...        ...        ...  \n",
       "1209595 -27.740000  28.400000 -27.469999  \n",
       "1209596 -27.889999  28.400000 -27.650000  \n",
       "1209597 -27.820000  28.400000 -27.600000  \n",
       "1209598 -27.180000  28.500000 -27.740000  \n",
       "1209599 -26.719999  28.299999 -27.670000  \n",
       "\n",
       "[1209600 rows x 180 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adres_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average power factor calculation\n",
    "\n",
    "Each household in the data set has six columns - P1, Q1, P2, Q2, P3, Q3.\n",
    "\n",
    "To compute the average power factor across the entire data set we will compute the average P across all the columns, and the average Q across all the columns. This can then be used to calculate an average power factor. \n",
    "\n",
    "As we can't make any assumptions about the relationship between the ADRES data set and the Pecan Street data this is all we will do.\n",
    "\n",
    "We can then use the average power factor to finish processing the Pecan Street data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average P: 215.26437624926513 W\n",
      "Average Q: 27.710495002939446 VAr\n",
      "Average S: 217.04060268828297 VA\n"
     ]
    }
   ],
   "source": [
    "# Even columns are P, odd are Q\n",
    "p_cols = [col for col in adres_df if col % 2 == 0]\n",
    "q_cols = [col for col in adres_df if col % 2 == 1]\n",
    "\n",
    "# Sum across columns then down the column\n",
    "p_sum = adres_df[p_cols].sum(axis=1).sum(axis=0)\n",
    "q_sum = adres_df[q_cols].sum(axis=1).sum(axis=0)\n",
    "\n",
    "# We've summed across the rows and the columns so need to divide by their length to get the average\n",
    "avg_p = p_sum / len(p_cols) / len(adres_df)\n",
    "avg_q = q_sum / len(q_cols) / len(adres_df)\n",
    "\n",
    "avg_s = math.sqrt((avg_p**2 + avg_q**2))\n",
    "\n",
    "print(f\"Average P: {avg_p} W\")\n",
    "print(f\"Average Q: {avg_q} VAr\")\n",
    "print(f\"Average S: {avg_s} VA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PF: 0.9918161559771888\n"
     ]
    }
   ],
   "source": [
    "avg_pf = avg_p / avg_s\n",
    "print(f\"Average PF: {avg_pf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have a power factor to use as our baseline\n",
    "Somewhat unsurprisingly, it's not far from unity. In general, residential premises typically operate near a unity power factor.\n",
    "\n",
    "Next step: use it to create feasible reactive power readings for each load and create the final input profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4a8b33f54e91e0fba0aee32d2cc6e29e6a619bb4eba0837462f8fb01455034b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('madpn': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
