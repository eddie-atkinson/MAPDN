{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook processes the input data for the simulation of an Australian distribution network feeder. Load and generation data is sourced from the [Pecan Street](https://www.pecanstreet.org/) student-licensed data set. Power factor data is sourced from the [ECO](https://www.vs.inf.ethz.ch/res/show.html?what=eco-data) data set and combined with the Pecan Street data to produce active and reactive power load profiles.\n",
    "\n",
    "Network models from the [Australian Low Voltage Feeder Taxonomy](https://near.csiro.au/assets/f325fb3c-2dcd-410c-97a8-e55dc68b8064) are converted from OpenDSS format to pandapower. \n",
    "\n",
    "The overall data cleaning process is depicted below:\n",
    "\n",
    "![data_cleaning](https://i.ibb.co/2KZ48xg/data-cleaning.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Processing\n",
    "The load data was downloaded from the Pecan Street Dataport. The original files were called `1s_data_austin_file{1,2,3,4}.csv.gz`. \n",
    "\n",
    "\n",
    "Each is unzipped and processed. The single file containing all households is split into a file per household containing only the `dataid`, `localminute`, `grid`, `solar` and `solar2` columns. This reduces the size of the data set, and makes it practical to manipulate the data from a single household at once in pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from scipy import io, stats\n",
    "import math\n",
    "from typing import Tuple, Any\n",
    "from datetime import datetime, timedelta\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PECAN_ST_FILENAMES = [\n",
    "    \"1s_data_austin_file1.csv.gz\",\n",
    "    \"1s_data_austin_file2.csv.gz\",\n",
    "    \"1s_data_austin_file3.csv.gz\",\n",
    "    \"1s_data_austin_file4.csv.gz\",\n",
    "]\n",
    "\n",
    "INPUT_DATA_PATH = Path(\"./input_data\")\n",
    "OUTPUT_DATA_PATH = Path(\"./output_data\")\n",
    "\n",
    "# Used for creating test and training sets later\n",
    "# Dates sourced from: https://www.calendardate.com/year2018.php\n",
    "SPRING_START = datetime(2018, 3, 20, 0, 0, 0)\n",
    "SPRING_END = datetime(2018, 6, 20, 0, 0, 0)\n",
    "\n",
    "SUMMER_START = datetime(2018, 6, 21, 0, 0)\n",
    "SUMMER_END = datetime(2018, 9, 21, 0, 0)\n",
    "\n",
    "AUTUMN_START = datetime(2018, 9, 22, 0, 0)\n",
    "AUTUMN_END = datetime(2018, 12, 20, 0, 0)\n",
    "\n",
    "START_OF_YEAR = datetime(2018, 1, 1, 0, 0)\n",
    "WINTER_START = datetime(2018, 12, 21, 0, 0)\n",
    "END_OF_YEAR = datetime(2018, 12, 31, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_household_file(file_path: Path, header_row: str):\n",
    "    with open(file_path, \"w\") as fd:\n",
    "        fd.write(header_row)\n",
    "\n",
    "\n",
    "def process_file(file_path: Path):\n",
    "    file_descriptors = {}\n",
    "    header_row = \"dataid,localminute,grid,solar,solar2\\n\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as infile:\n",
    "            csv_reader = csv.DictReader(infile)\n",
    "            for row in csv_reader:\n",
    "                dataid = row[\"dataid\"]\n",
    "                localminute = row[\"localminute\"]\n",
    "                grid = row[\"grid\"]\n",
    "                solar = row[\"solar\"]\n",
    "                solar2 = row[\"solar2\"]\n",
    "\n",
    "                file_descriptor = file_descriptors.get(dataid, None)\n",
    "                if not file_descriptor:\n",
    "                    output_file_path = OUTPUT_DATA_PATH / f\"{dataid}-load-pv.csv\"\n",
    "                    if not output_file_path.exists():\n",
    "                        create_household_file(output_file_path, header_row)\n",
    "                    file_descriptor = open(output_file_path, \"a\")\n",
    "                    file_descriptors[dataid] = file_descriptor\n",
    "                file_descriptor.write(\n",
    "                    f\"{dataid},{localminute},{grid},{solar},{solar2}\\n\",\n",
    "                )\n",
    "    finally:\n",
    "        for fd in file_descriptors.values():\n",
    "            fd.close()\n",
    "\n",
    "\n",
    "def unzip_raw_data(data_path: Path, unzip_path: Path):\n",
    "    if not unzip_path.exists():\n",
    "        # Lord forgive me for this but Python is terribly slow at this\n",
    "        os.system(f\"gzip -d < {data_path.resolve()} > {unzip_path.resolve()}\")\n",
    "        print(\"unzipped\")\n",
    "        \n",
    "\n",
    "\n",
    "def process_data(data_path: Path):\n",
    "    outfile_name = data_path.stem\n",
    "    unzipped_path = INPUT_DATA_PATH / outfile_name\n",
    "    unzip_raw_data(data_path, unzipped_path)\n",
    "    process_file(unzipped_path)\n",
    "    unzipped_path.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This takes a long time to run, 1.5 hours on an M1 mac.\n",
    "# I could probably optimise it, but I'm only running it once \n",
    "# Should probably also multi thread it\n",
    "# for file_name in PECAN_ST_FILENAMES:\n",
    "#     print(file_name)\n",
    "#     data_path = INPUT_DATA_PATH / file_name\n",
    "#     process_data(data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate to 30s resolution\n",
    "There's still a lot of data to work with. Though a resolution of 1s is interesting, the volume of data is too large for us to work with sensibly. Let's downsample the data to 30s resolution to make it a bit easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_interval(dt):\n",
    "    seconds = dt.second\n",
    "    interval_args = {\n",
    "        \"year\": dt.year,\n",
    "        \"month\": dt.month,\n",
    "        \"day\": dt.day,\n",
    "        \"hour\": dt.hour,\n",
    "        \"minute\": dt.minute,\n",
    "    }\n",
    "\n",
    "    if seconds < 30:\n",
    "        return datetime(**interval_args, second=0)\n",
    "    else:\n",
    "        return datetime(**interval_args, second=30)\n",
    "\n",
    "\n",
    "def calculate_use(grid: str, solar: str, solar2: str) -> Union[float, None]:\n",
    "    if not grid:\n",
    "        return None\n",
    "    grid_val = float(grid)\n",
    "    solar_val = float(solar) if solar else 0\n",
    "    solar2_val = float(solar2) if solar2 else 0\n",
    "\n",
    "    # This may seem weird but it's explained here: https://docs.google.com/document/d/1_9H9N4cgKmJho7hK8nii6flIGKPycL7tlWEtd4UhVEQ/edit#\n",
    "    return grid_val + solar_val + solar2_val\n",
    "\n",
    "\n",
    "def calculate_net_solar(solar: str, solar2: str) -> Union[float, None]:\n",
    "    if not solar and not solar2:\n",
    "        return None\n",
    "    solar_val = float(solar) if solar else 0\n",
    "    solar2_val = float(solar) if solar2 else 0\n",
    "\n",
    "    return solar_val + solar2_val\n",
    "\n",
    "\n",
    "def downsample_data(file_path: Path, out_path: Path):\n",
    "    data_periods = {}\n",
    "    with open(file_path, \"r\") as infile:\n",
    "        csv_reader = csv.DictReader(infile)\n",
    "        for line in csv_reader:\n",
    "            # The last three characters are TZ info, we will lose an hour's data every time the clocks change.\n",
    "            # Fortunately, we don't really care about that\n",
    "            localminute = line[\"localminute\"][:-3]\n",
    "            grid = line[\"grid\"]\n",
    "            solar = line[\"solar\"]\n",
    "            solar2 = line[\"solar2\"]\n",
    "            solar_val = calculate_net_solar(solar, solar2)\n",
    "            use = calculate_use(grid, solar, solar2)\n",
    "            dt = datetime.strptime(localminute, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            interval = get_closest_interval(dt)\n",
    "            if not data_periods.get(interval, None):\n",
    "                data_periods[interval] = dict(\n",
    "                    solar_sum=0, solar_count=0, use_sum=0, use_count=0\n",
    "                )\n",
    "\n",
    "            data = data_periods[interval]\n",
    "            if solar_val is not None:\n",
    "                data[\"solar_sum\"] += solar_val\n",
    "                data[\"solar_count\"] += 1\n",
    "            if use is not None:\n",
    "                data[\"use_sum\"] += use\n",
    "                data[\"use_count\"] += 1\n",
    "\n",
    "    intervals = sorted(data_periods)\n",
    "    with open(out_path, \"w\") as outfile:\n",
    "        header = \"datetime,solar,use\\n\"\n",
    "        outfile.write(header)\n",
    "        for interval in intervals:\n",
    "            interval_data = data_periods[interval]\n",
    "            iso_date = interval.isoformat()\n",
    "            solar_count = interval_data[\"solar_count\"]\n",
    "            solar_sum = interval_data[\"solar_sum\"]\n",
    "\n",
    "            use_count = interval_data[\"use_count\"]\n",
    "            use_sum = interval_data[\"use_sum\"]\n",
    "\n",
    "            solar_val = \"\"\n",
    "            use_val = \"\"\n",
    "            if solar_count:\n",
    "                solar_val = solar_sum / solar_count\n",
    "            if use_count:\n",
    "                use_val = use_sum / use_count\n",
    "            outfile.write(f\"{iso_date},{solar_val},{use_val}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This also takes ages - should really have written this more effficiently\n",
    "threads = 25\n",
    "t = ThreadPool(threads)\n",
    "\n",
    "files_to_process = []\n",
    "for file_path in OUTPUT_DATA_PATH.iterdir():\n",
    "    if not str(file_path).endswith(\"load-pv.csv\"):\n",
    "        continue\n",
    "    outfile_name = f\"{file_path.stem}-reduced.csv\"\n",
    "    outfile_path = file_path.parent / outfile_name\n",
    "    files_to_process.append((\n",
    "        file_path,\n",
    "        outfile_path\n",
    "    ))\n",
    " \n",
    "\n",
    "with ThreadPool(threads) as t:\n",
    "    t.starmap(downsample_data, files_to_process)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further processing\n",
    "The next steps are:\n",
    "1. Aggregate the ECO dataset and compute an average power factor.\n",
    "2. Check the existing load and PV data for outliers and replace them with neighbouring values. We know from experience that these values exist. \n",
    "3. Perturb the average power factor by +/- 10% and compute the appropriate reactive power for each interval in the load data.\n",
    "4. Bootstrap to 100 load and PV profiles\n",
    "5. Create active, reactive, and pv profiles for all the households.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data was requested from ETH Zurich and downloaded directly from their portal\n",
    "adres_input_path = INPUT_DATA_PATH / \"ADRES_Daten_120208.mat\"\n",
    "loaded = io.loadmat(adres_input_path)\n",
    "adres_df = pd.DataFrame(loaded[\"Data\"][\"PQ\"][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.200001</td>\n",
       "      <td>-14.670000</td>\n",
       "      <td>22.150000</td>\n",
       "      <td>3.189000</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>-80.730003</td>\n",
       "      <td>550.083313</td>\n",
       "      <td>-31.236666</td>\n",
       "      <td>247.800003</td>\n",
       "      <td>66.066666</td>\n",
       "      <td>...</td>\n",
       "      <td>168.100006</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>-31.100000</td>\n",
       "      <td>52.200001</td>\n",
       "      <td>-42.900002</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>-1.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.500000</td>\n",
       "      <td>-14.980000</td>\n",
       "      <td>22.180000</td>\n",
       "      <td>3.529000</td>\n",
       "      <td>140.699997</td>\n",
       "      <td>-80.550003</td>\n",
       "      <td>549.080017</td>\n",
       "      <td>-31.166000</td>\n",
       "      <td>247.960007</td>\n",
       "      <td>66.199997</td>\n",
       "      <td>...</td>\n",
       "      <td>168.500000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>10.640000</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>54.900002</td>\n",
       "      <td>-31.180000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>-43.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>-1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.799999</td>\n",
       "      <td>-15.120000</td>\n",
       "      <td>22.040001</td>\n",
       "      <td>3.408000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>-80.330002</td>\n",
       "      <td>549.099976</td>\n",
       "      <td>-31.172001</td>\n",
       "      <td>247.880005</td>\n",
       "      <td>65.839996</td>\n",
       "      <td>...</td>\n",
       "      <td>168.100006</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>10.810000</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>55.799999</td>\n",
       "      <td>-31.299999</td>\n",
       "      <td>52.900002</td>\n",
       "      <td>-43.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>-1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.900002</td>\n",
       "      <td>-14.660000</td>\n",
       "      <td>22.219999</td>\n",
       "      <td>2.956000</td>\n",
       "      <td>141.100006</td>\n",
       "      <td>-80.550003</td>\n",
       "      <td>540.880005</td>\n",
       "      <td>-31.386000</td>\n",
       "      <td>247.399994</td>\n",
       "      <td>65.739998</td>\n",
       "      <td>...</td>\n",
       "      <td>168.500000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>10.710000</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>60.200001</td>\n",
       "      <td>-31.660000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>-42.900002</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>-1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.700001</td>\n",
       "      <td>-14.860000</td>\n",
       "      <td>22.120001</td>\n",
       "      <td>3.148000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>-80.669998</td>\n",
       "      <td>428.760010</td>\n",
       "      <td>-29.719999</td>\n",
       "      <td>247.259995</td>\n",
       "      <td>65.699997</td>\n",
       "      <td>...</td>\n",
       "      <td>168.399994</td>\n",
       "      <td>20.700001</td>\n",
       "      <td>10.650000</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>60.200001</td>\n",
       "      <td>-31.740000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>-42.900002</td>\n",
       "      <td>35.099998</td>\n",
       "      <td>-1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209595</th>\n",
       "      <td>102.300003</td>\n",
       "      <td>24.510000</td>\n",
       "      <td>133.800003</td>\n",
       "      <td>-62.160000</td>\n",
       "      <td>108.300003</td>\n",
       "      <td>54.270000</td>\n",
       "      <td>290.899994</td>\n",
       "      <td>17.570000</td>\n",
       "      <td>107.699997</td>\n",
       "      <td>40.610001</td>\n",
       "      <td>...</td>\n",
       "      <td>139.899994</td>\n",
       "      <td>-9.500000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>2.300</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>-7.800000</td>\n",
       "      <td>34.200001</td>\n",
       "      <td>-27.740000</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>-27.469999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209596</th>\n",
       "      <td>102.500000</td>\n",
       "      <td>24.459999</td>\n",
       "      <td>133.800003</td>\n",
       "      <td>-62.169998</td>\n",
       "      <td>113.400002</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>289.799988</td>\n",
       "      <td>16.240000</td>\n",
       "      <td>107.699997</td>\n",
       "      <td>40.669998</td>\n",
       "      <td>...</td>\n",
       "      <td>139.800003</td>\n",
       "      <td>-9.500000</td>\n",
       "      <td>221.699997</td>\n",
       "      <td>2.300</td>\n",
       "      <td>57.200001</td>\n",
       "      <td>-7.900000</td>\n",
       "      <td>34.200001</td>\n",
       "      <td>-27.889999</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>-27.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209597</th>\n",
       "      <td>102.699997</td>\n",
       "      <td>24.549999</td>\n",
       "      <td>133.800003</td>\n",
       "      <td>-62.349998</td>\n",
       "      <td>112.300003</td>\n",
       "      <td>54.090000</td>\n",
       "      <td>287.299988</td>\n",
       "      <td>13.430000</td>\n",
       "      <td>107.900002</td>\n",
       "      <td>40.720001</td>\n",
       "      <td>...</td>\n",
       "      <td>139.800003</td>\n",
       "      <td>-9.600000</td>\n",
       "      <td>221.800003</td>\n",
       "      <td>2.400</td>\n",
       "      <td>56.299999</td>\n",
       "      <td>-7.700000</td>\n",
       "      <td>34.200001</td>\n",
       "      <td>-27.820000</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>-27.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209598</th>\n",
       "      <td>102.500000</td>\n",
       "      <td>24.389999</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>-62.290001</td>\n",
       "      <td>108.199997</td>\n",
       "      <td>54.189999</td>\n",
       "      <td>282.299988</td>\n",
       "      <td>11.320000</td>\n",
       "      <td>107.599998</td>\n",
       "      <td>40.020000</td>\n",
       "      <td>...</td>\n",
       "      <td>139.899994</td>\n",
       "      <td>-9.500000</td>\n",
       "      <td>221.500000</td>\n",
       "      <td>2.400</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>-7.700000</td>\n",
       "      <td>34.299999</td>\n",
       "      <td>-27.180000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>-27.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209599</th>\n",
       "      <td>102.199997</td>\n",
       "      <td>24.530001</td>\n",
       "      <td>134.100006</td>\n",
       "      <td>-62.279999</td>\n",
       "      <td>108.199997</td>\n",
       "      <td>54.189999</td>\n",
       "      <td>280.500000</td>\n",
       "      <td>10.540000</td>\n",
       "      <td>107.699997</td>\n",
       "      <td>39.919998</td>\n",
       "      <td>...</td>\n",
       "      <td>139.899994</td>\n",
       "      <td>-9.300000</td>\n",
       "      <td>221.600006</td>\n",
       "      <td>2.500</td>\n",
       "      <td>56.299999</td>\n",
       "      <td>-7.600000</td>\n",
       "      <td>34.299999</td>\n",
       "      <td>-26.719999</td>\n",
       "      <td>28.299999</td>\n",
       "      <td>-27.670000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1209600 rows Ã— 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0          1           2          3           4          5    \\\n",
       "0         40.200001 -14.670000   22.150000   3.189000  141.500000 -80.730003   \n",
       "1         41.500000 -14.980000   22.180000   3.529000  140.699997 -80.550003   \n",
       "2         40.799999 -15.120000   22.040001   3.408000  141.000000 -80.330002   \n",
       "3         40.900002 -14.660000   22.219999   2.956000  141.100006 -80.550003   \n",
       "4         40.700001 -14.860000   22.120001   3.148000  141.000000 -80.669998   \n",
       "...             ...        ...         ...        ...         ...        ...   \n",
       "1209595  102.300003  24.510000  133.800003 -62.160000  108.300003  54.270000   \n",
       "1209596  102.500000  24.459999  133.800003 -62.169998  113.400002  54.000000   \n",
       "1209597  102.699997  24.549999  133.800003 -62.349998  112.300003  54.090000   \n",
       "1209598  102.500000  24.389999  134.000000 -62.290001  108.199997  54.189999   \n",
       "1209599  102.199997  24.530001  134.100006 -62.279999  108.199997  54.189999   \n",
       "\n",
       "                6          7           8          9    ...         170  \\\n",
       "0        550.083313 -31.236666  247.800003  66.066666  ...  168.100006   \n",
       "1        549.080017 -31.166000  247.960007  66.199997  ...  168.500000   \n",
       "2        549.099976 -31.172001  247.880005  65.839996  ...  168.100006   \n",
       "3        540.880005 -31.386000  247.399994  65.739998  ...  168.500000   \n",
       "4        428.760010 -29.719999  247.259995  65.699997  ...  168.399994   \n",
       "...             ...        ...         ...        ...  ...         ...   \n",
       "1209595  290.899994  17.570000  107.699997  40.610001  ...  139.899994   \n",
       "1209596  289.799988  16.240000  107.699997  40.669998  ...  139.800003   \n",
       "1209597  287.299988  13.430000  107.900002  40.720001  ...  139.800003   \n",
       "1209598  282.299988  11.320000  107.599998  40.020000  ...  139.899994   \n",
       "1209599  280.500000  10.540000  107.699997  39.919998  ...  139.899994   \n",
       "\n",
       "               171         172    173        174        175        176  \\\n",
       "0        20.600000   10.700000 -0.995  55.000000 -31.100000  52.200001   \n",
       "1        20.600000   10.640000 -0.947  54.900002 -31.180000  52.500000   \n",
       "2        20.600000   10.810000 -0.945  55.799999 -31.299999  52.900002   \n",
       "3        20.600000   10.710000 -0.962  60.200001 -31.660000  52.500000   \n",
       "4        20.700001   10.650000 -0.927  60.200001 -31.740000  52.500000   \n",
       "...            ...         ...    ...        ...        ...        ...   \n",
       "1209595  -9.500000  222.000000  2.300  57.500000  -7.800000  34.200001   \n",
       "1209596  -9.500000  221.699997  2.300  57.200001  -7.900000  34.200001   \n",
       "1209597  -9.600000  221.800003  2.400  56.299999  -7.700000  34.200001   \n",
       "1209598  -9.500000  221.500000  2.400  56.500000  -7.700000  34.299999   \n",
       "1209599  -9.300000  221.600006  2.500  56.299999  -7.600000  34.299999   \n",
       "\n",
       "               177        178        179  \n",
       "0       -42.900002  35.000000  -1.700000  \n",
       "1       -43.000000  35.000000  -1.800000  \n",
       "2       -43.000000  35.000000  -1.800000  \n",
       "3       -42.900002  35.000000  -1.800000  \n",
       "4       -42.900002  35.099998  -1.900000  \n",
       "...            ...        ...        ...  \n",
       "1209595 -27.740000  28.400000 -27.469999  \n",
       "1209596 -27.889999  28.400000 -27.650000  \n",
       "1209597 -27.820000  28.400000 -27.600000  \n",
       "1209598 -27.180000  28.500000 -27.740000  \n",
       "1209599 -26.719999  28.299999 -27.670000  \n",
       "\n",
       "[1209600 rows x 180 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adres_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average power factor calculation\n",
    "\n",
    "Each household in the data set has six columns - P1, Q1, P2, Q2, P3, Q3.\n",
    "\n",
    "To compute the average power factor across the entire data set we will compute the average P across all the columns, and the average Q across all the columns. This can then be used to calculate an average power factor. \n",
    "\n",
    "As we can't make any assumptions about the relationship between the ADRES data set and the Pecan Street data this is all we will do.\n",
    "\n",
    "We can then use the average power factor to finish processing the Pecan Street data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average P: 215.26437624926513 W\n",
      "Average Q: 27.710495002939446 VAr\n",
      "Average S: 217.04060268828297 VA\n"
     ]
    }
   ],
   "source": [
    "# Even columns are P, odd are Q\n",
    "p_cols = [col for col in adres_df if col % 2 == 0]\n",
    "q_cols = [col for col in adres_df if col % 2 == 1]\n",
    "\n",
    "# Sum across columns then down the column\n",
    "p_sum = adres_df[p_cols].sum(axis=1).sum(axis=0)\n",
    "q_sum = adres_df[q_cols].sum(axis=1).sum(axis=0)\n",
    "\n",
    "# We've summed across the rows and the columns so need to divide by their length to get the average\n",
    "avg_p = p_sum / len(p_cols) / len(adres_df)\n",
    "avg_q = q_sum / len(q_cols) / len(adres_df)\n",
    "\n",
    "avg_s = math.sqrt((avg_p**2 + avg_q**2))\n",
    "\n",
    "print(f\"Average P: {avg_p} W\")\n",
    "print(f\"Average Q: {avg_q} VAr\")\n",
    "print(f\"Average S: {avg_s} VA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PF: 0.9918161559771888\n"
     ]
    }
   ],
   "source": [
    "avg_pf = avg_p / avg_s\n",
    "print(f\"Average PF: {avg_pf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have a power factor to use as our baseline\n",
    "Somewhat unsurprisingly, it's not far from unity (0.9918161559771888). In general, residential premises typically operate near a unity power factor.\n",
    "\n",
    "Next step: use it to create feasible reactive power readings for each load and create the final input profiles.\n",
    "\n",
    "We will need to do the following:\n",
    "1. Filter out values that are 5 standard deviations from the mean, they are likely measurement errors and will distort the data. \n",
    "2. Fill in missing values. For this we will be replacing missing intervals with the closest interval at that time of day with a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DATE = datetime(2018, 1, 1, 0, 0, 0)\n",
    "MAX_DATE = datetime(2018, 12, 31, 23, 59, 30)\n",
    "AVG_SOLAR_SYSTEM_KW = 6.9\n",
    "\n",
    "\n",
    "def add_missing_periods(df, freq=\"30S\"):\n",
    "    date_range = pd.date_range(MIN_DATE, MAX_DATE, freq=freq)\n",
    "    df = df.reindex(date_range, fill_value=np.nan)\n",
    "    df[\"datetime\"] = df.index\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_na_proportion(df) -> Any:\n",
    "    solar_proportion = df[\"solar\"].isna().sum() / len(df[\"solar\"])\n",
    "    use_proportion = df[\"use\"].isna().sum() / len(df[\"use\"])\n",
    "    return {\"solar\": solar_proportion, \"use\": use_proportion}\n",
    "\n",
    "\n",
    "def remove_outliers(df, column, sigma_threshold=5):\n",
    "    outliers = np.abs(stats.zscore(df[column], nan_policy=\"omit\")) > sigma_threshold\n",
    "    df.loc[outliers, column] = np.nan\n",
    "    # Only fill at max 5 intervals forward\n",
    "    df[column] = df[column].fillna(method=\"ffill\", limit=5)\n",
    "\n",
    "\n",
    "def get_replacement_value(df, index, column):\n",
    "    min_date = np.min(df[\"datetime\"]).to_pydatetime()\n",
    "    max_date = np.max(df[\"datetime\"]).to_pydatetime()\n",
    "\n",
    "    row = df.loc[index]\n",
    "\n",
    "    row_datetime = row[\"datetime\"].to_pydatetime()\n",
    "\n",
    "    first_relevant_datetime = min_date.replace(\n",
    "        hour=row_datetime.hour,\n",
    "        minute=row_datetime.minute,\n",
    "        second=row_datetime.second,\n",
    "    )\n",
    "    last_relevant_datetime = max_date.replace(\n",
    "        hour=row_datetime.hour,\n",
    "        minute=row_datetime.minute,\n",
    "        second=row_datetime.second,\n",
    "    )\n",
    "\n",
    "    forward_dt = row_datetime\n",
    "    back_dt = row_datetime\n",
    "\n",
    "    replacement_val = np.nan\n",
    "\n",
    "    while forward_dt <= last_relevant_datetime or back_dt >= first_relevant_datetime:\n",
    "        if back_dt in df.index and not np.isnan(df.loc[back_dt, column]):\n",
    "            replacement_val = df.loc[back_dt, column]\n",
    "            break\n",
    "\n",
    "        if forward_dt in df.index and not np.isnan(df.loc[forward_dt, column]):\n",
    "            replacement_val = df.loc[forward_dt, column]\n",
    "            break\n",
    "\n",
    "        forward_dt += timedelta(days=1)\n",
    "        back_dt -= timedelta(days=1)\n",
    "\n",
    "    if np.isnan(replacement_val):\n",
    "        raise ValueError(f\"Couldn't find replacement values for date: {index}\")\n",
    "\n",
    "    return replacement_val\n",
    "\n",
    "\n",
    "def replace_missing_values(df, column):\n",
    "    indices_to_replace = df.loc[df[column].isna(), column].index\n",
    "    replacement_values = {}\n",
    "    # This is O(N^2) in the worst case (which could be pretty bad)\n",
    "    # Fortunately for us our data is pretty complete\n",
    "    for index in indices_to_replace:\n",
    "        replacement_val = get_replacement_value(df, index, column)\n",
    "        replacement_values[index] = replacement_val\n",
    "    df[column] = df[column].fillna(value=replacement_values)\n",
    "\n",
    "\n",
    "def normalise_solar(df):\n",
    "    # Remove all values less than 0\n",
    "    # Parasitics are important, but not *that* important\n",
    "    negative_vals = df[\"solar\"] < 0\n",
    "    df.loc[negative_vals, \"solar\"] = 0\n",
    "    abs_solar = np.abs(df[\"solar\"])\n",
    "    max_val = np.max(abs_solar)\n",
    "    # Need to produce outputs in MW not kW \n",
    "    df[\"solar\"] = (df[\"solar\"] / max_val) * AVG_SOLAR_SYSTEM_KW / 1000\n",
    "\n",
    "\n",
    "def add_reactive(df):\n",
    "    power_factor = pd.Series([avg_pf] * len(df), index=df.index)\n",
    "    random_perturbation = np.random.uniform(0.9, 1.0, size=len(df))\n",
    "    power_factor *= random_perturbation\n",
    "    real_power = df[\"use\"]\n",
    "    apparent_power = real_power / power_factor\n",
    "    # Multiplication is faster than exponentiation here\n",
    "    s_squared = apparent_power * apparent_power\n",
    "    p_squared = real_power * real_power\n",
    "    q_squared = s_squared - p_squared\n",
    "    df[\"use_reactive\"] = np.sqrt(q_squared)\n",
    "    df.rename(columns={\"use\": \"use_active\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_site_data(file_path) -> Tuple[pd.DataFrame, Any]:\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        parse_dates=[\"datetime\"],\n",
    "        dtype={\n",
    "            \"use\": float,\n",
    "            \"solar\": float,\n",
    "        },\n",
    "    )\n",
    "    df = df.set_index(df[\"datetime\"])\n",
    "    df = add_missing_periods(df)\n",
    "\n",
    "    na_proportions = get_na_proportion(df)\n",
    "    site_has_solar = na_proportions[\"solar\"] != 1.0\n",
    "\n",
    "    remove_outliers(df, \"solar\")\n",
    "    remove_outliers(df, \"use\")\n",
    "\n",
    "    if site_has_solar:\n",
    "        replace_missing_values(df, \"solar\")\n",
    "        normalise_solar(df)\n",
    "\n",
    "    replace_missing_values(df, \"use\")\n",
    "    # Convert use from kW to MW for pandapower profiles\n",
    "    df[\"use\"] = df[\"use\"] / 1000\n",
    "\n",
    "    add_reactive(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df, na_proportions\n",
    "\n",
    "\n",
    "household_na_proportions = []\n",
    "household_index_map = []\n",
    "\n",
    "def create_agg_profiles(active: pd.DataFrame, reactive: pd.DataFrame, pv: pd.DataFrame):\n",
    "    load_index = 0\n",
    "    solar_index = 0\n",
    "    for file_path in OUTPUT_DATA_PATH.iterdir():\n",
    "        site_id = file_path.stem.split(\"-\")[0]\n",
    "        if not str(file_path).endswith(\"load-pv-reduced.csv\"):\n",
    "            continue\n",
    "\n",
    "        df, na_proportions = process_site_data(file_path)\n",
    "\n",
    "        site_has_solar = na_proportions[\"solar\"] != 1.0\n",
    "\n",
    "        reactive[load_index] = df[\"use_reactive\"]\n",
    "        active[load_index] = df[\"use_active\"]\n",
    "\n",
    "        if site_has_solar:\n",
    "            pv[solar_index] = df[\"solar\"]\n",
    "\n",
    "        index_map = {\"household_id\": site_id, \"load_index\": load_index}\n",
    "\n",
    "        load_index += 1\n",
    "        if site_has_solar:\n",
    "            index_map[\"solar_index\"] = solar_index\n",
    "            solar_index += 1\n",
    "\n",
    "        household_index_map.append(index_map)\n",
    "        household_na_proportions.append(\n",
    "            {\n",
    "                \"household_id\": site_id,\n",
    "                \"solar_na\": na_proportions[\"solar\"],\n",
    "                \"use_na\": na_proportions[\"use\"],\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for pertubing active power\n",
    "random.seed(42)\n",
    "reactive = pd.DataFrame()\n",
    "active = pd.DataFrame()\n",
    "pv = pd.DataFrame()\n",
    "\n",
    "\n",
    "date_range = pd.date_range(MIN_DATE, MAX_DATE, freq=\"30S\")\n",
    "reactive[\"datetime\"] = date_range\n",
    "active[\"datetime\"] = date_range\n",
    "pv[\"datetime\"] = date_range\n",
    "\n",
    "# Pretty slow again, sorry\n",
    "create_agg_profiles(active, reactive, pv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to keep track of which household is mapped to which index in the final data set\n",
    "household_map_df = pd.DataFrame(household_index_map) \n",
    "na_proportion_df = pd.DataFrame(household_na_proportions)\n",
    "\n",
    "household_map_df.to_csv(OUTPUT_DATA_PATH / \"household_index_map.csv\", index=False)\n",
    "na_proportion_df.to_csv(OUTPUT_DATA_PATH / \"na_proportion.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping time!\n",
    "Now we need to bootstrap to the 109 profiles required for the LVFT models.\n",
    "\n",
    "We will employ block bootstrapping, building up new load profiles by selecting a day of data at random from our original profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_int(val):\n",
    "    try:\n",
    "        int(val)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_days_data(df, date, column):\n",
    "    lower_datetime = pd.to_datetime(date)\n",
    "    upper_datetime = lower_datetime.replace(hour=23, minute=59, second=59)\n",
    "    mask = (df.index >= lower_datetime) & (df.index < upper_datetime)\n",
    "    return df.loc[mask, column]\n",
    "\n",
    "\n",
    "def get_bootstrap_samples(df, n_profiles):\n",
    "    df = df.set_index(\"datetime\")\n",
    "    profiles_to_sample_from = [col for col in df.columns if is_int(col)]\n",
    "    samples_to_take = []\n",
    "    dates_to_sample = sorted(df.index.map(pd.Timestamp.date).unique())\n",
    "    for _ in range(n_profiles):\n",
    "        sample_columns = random.choices(profiles_to_sample_from, k=len(dates_to_sample))\n",
    "        samples_to_take.append(list(zip(dates_to_sample, sample_columns)))\n",
    "    df = df.reset_index(drop=True)\n",
    "    return samples_to_take\n",
    "\n",
    "\n",
    "def bootstrap_samples(df, samples_to_take, first_index):\n",
    "    df = df.set_index(\"datetime\")\n",
    "    idx = first_index\n",
    "    for sample in samples_to_take:\n",
    "        data_sample = None\n",
    "        for date, sample_column in sample:\n",
    "            data = get_days_data(df, date, sample_column)\n",
    "            if data_sample is None:\n",
    "                data_sample = data\n",
    "            else:\n",
    "                data_sample = pd.concat([data_sample, data])\n",
    "        data_sample = data_sample.rename(idx)\n",
    "        df = df.join(data_sample)\n",
    "        idx += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for bootstrapping samples\n",
    "random.seed(42)\n",
    "load_bootstrap_samples = get_bootstrap_samples(reactive, 84)\n",
    "reactive_df = bootstrap_samples(reactive, load_bootstrap_samples, 25)\n",
    "active_df = bootstrap_samples(active, load_bootstrap_samples, 25)\n",
    "\n",
    "pv_bootstrap_samples = get_bootstrap_samples(pv, 91)\n",
    "pv_df = bootstrap_samples(pv, pv_bootstrap_samples, 19)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to create a test-train split\n",
    " \n",
    "Similar to the bootstrapping we will sample in blocks for the test-train split. Given that the data is time series it makes little intuitive sense to randomly sample rows. Instead 14 days are randomly selected from each season to create a test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_data(df, date, column):\n",
    "    lower_datetime = pd.to_datetime(date)\n",
    "    upper_datetime = lower_datetime.replace(hour=23, minute=59, second=59)\n",
    "    mask = (df.index >= lower_datetime) & (df.index < upper_datetime)\n",
    "    return df.loc[mask, column]\n",
    "\n",
    "\n",
    "def get_dates_in_range(*args):\n",
    "    if len(args) % 2 != 0 or not len(args):\n",
    "        raise ValueError(\"Date ranges must be specified in pairs\")\n",
    "    date_range = []\n",
    "    pairs = [(args[i], args[i + 1]) for i in range(0, len(args) - 1, 2)]\n",
    "    for start, end in pairs:\n",
    "        pair_range = list(pd.date_range(start, end, freq=\"1D\"))\n",
    "        date_range += pair_range\n",
    "\n",
    "    return date_range\n",
    "\n",
    "\n",
    "def sample_n_from_each_collection(collections, n):\n",
    "    samples = []\n",
    "    for collection in collections:\n",
    "        samples += random.sample(collection, n)\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def create_test_train_split(df, test_dates):\n",
    "    test_indices = None\n",
    "    # We don't care which column we select, we just care about the index\n",
    "    col_to_select = df.columns[0]\n",
    "    for test_date in test_dates:\n",
    "        days_data = get_days_data(df, test_date, col_to_select)\n",
    "        if test_indices is None:\n",
    "            test_indices = days_data\n",
    "        else:\n",
    "            test_indices = pd.concat([test_indices, days_data])\n",
    "    test_indices = test_indices.index\n",
    "    train_indices = ~df.index.isin(test_indices)\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.loc[train_indices]\n",
    "    return test_df, train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "spring_dates = get_dates_in_range(SPRING_START, SPRING_END)\n",
    "summer_dates = get_dates_in_range(SUMMER_START, SUMMER_END)\n",
    "autumn_dates = get_dates_in_range(AUTUMN_START, AUTUMN_END)\n",
    "winter_dates = get_dates_in_range(\n",
    "    START_OF_YEAR, SPRING_START, WINTER_START, END_OF_YEAR\n",
    ")\n",
    "\n",
    "test_dates = sample_n_from_each_collection(\n",
    "    [spring_dates, summer_dates, autumn_dates, winter_dates], 14\n",
    ")\n",
    "\n",
    "test_reactive_df, train_reactive_df = create_test_train_split(reactive_df, test_dates)\n",
    "test_active_df, train_active_df = create_test_train_split(active_df, test_dates)\n",
    "test_pv_df, train_pv_df = create_test_train_split(pv_df, test_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the indices before saving\n",
    "test_reactive_df = test_reactive_df.reset_index()\n",
    "train_reactive_df = train_reactive_df.reset_index()\n",
    "\n",
    "test_active_df = test_active_df.reset_index()\n",
    "train_active_df = train_active_df.reset_index()\n",
    "\n",
    "test_pv_df = test_pv_df.reset_index()\n",
    "train_pv_df = train_pv_df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reactive_df.to_csv(OUTPUT_DATA_PATH / \"test_reactive.csv\", index=False)\n",
    "train_reactive_df.to_csv(OUTPUT_DATA_PATH / \"train_reactive.csv\", index=False)\n",
    "\n",
    "test_active_df.to_csv(OUTPUT_DATA_PATH / \"test_active.csv\", index=False)\n",
    "train_active_df.to_csv(OUTPUT_DATA_PATH / \"train_active.csv\", index=False)\n",
    "\n",
    "test_pv_df.to_csv(OUTPUT_DATA_PATH / \"test_pv.csv\", index=False)\n",
    "train_pv_df.to_csv(OUTPUT_DATA_PATH / \"train_pv.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "We have 109 PV, active power and reactive power data sets as inputs. \n",
    "\n",
    "Now let's do the network models (in another notebook)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4a8b33f54e91e0fba0aee32d2cc6e29e6a619bb4eba0837462f8fb01455034b"
  },
  "kernelspec": {
   "display_name": "mapdn:Python",
   "language": "python",
   "name": "conda-env-mapdn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
